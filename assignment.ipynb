{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ac7b76",
   "metadata": {},
   "source": [
    "### Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e43a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import mnist\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e9b25",
   "metadata": {},
   "source": [
    "### Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b45b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45531eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Obtaining dependency information for pytorch-lightning from https://files.pythonhosted.org/packages/e9/3c/8d05e269011d43c35b1fb3baca8d88bac6668f6c4bde565ad9ed27000b22/pytorch_lightning-2.2.2-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.2.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (2023.3.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
      "  Obtaining dependency information for torchmetrics>=0.7.0 from https://files.pythonhosted.org/packages/f3/0e/cedcb9c8aeb2d1f655f8d05f841b14d84b0a68d9f31afae4af55c7c6d0a9/torchmetrics-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (4.7.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
      "  Obtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/5e/9e/e7768a8e363fc6f0c978bb7a0aa7641f10d80be60000e788ef2f01d34a7c/lightning_utilities-0.11.2-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (3.9.0)\n",
      "Collecting typing-extensions>=4.4.0 (from pytorch-lightning)\n",
      "  Obtaining dependency information for typing-extensions>=4.4.0 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
      "Downloading pytorch_lightning-2.2.2-py3-none-any.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m715.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m612.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: typing-extensions, lightning-utilities, torchmetrics, pytorch-lightning\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed lightning-utilities-0.11.2 pytorch-lightning-2.2.2 torchmetrics-1.3.2 typing-extensions-4.11.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch-lightning\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "class LitMNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b51172",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49dc79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /Users/4lch3m1st/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 9912422/9912422 [00:25<00:00, 384546.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/MNIST/raw/train-images-idx3-ubyte.gz to /Users/4lch3m1st/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /Users/4lch3m1st/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 28881/28881 [00:00<00:00, 49316.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/MNIST/raw/train-labels-idx1-ubyte.gz to /Users/4lch3m1st/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /Users/4lch3m1st/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1648877/1648877 [00:03<00:00, 548699.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/MNIST/raw/t10k-images-idx3-ubyte.gz to /Users/4lch3m1st/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /Users/4lch3m1st/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 4542/4542 [00:00<00:00, 205678.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/MNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/4lch3m1st/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load MNIST dataset\n",
    "dataset = datasets.MNIST(root=os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecb6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define encoder layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through encoder\n",
    "        return encoded_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80841bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Define decoder layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through decoder\n",
    "        return decoded_data\n",
    "\n",
    "# LightningModule\n",
    "class LitAutoEncoder(LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(LitAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, _ = batch  \n",
    "        reconstructed = self(x)\n",
    "        loss = nn.MSELoss()(reconstructed, x)  \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9dc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd611012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/4lch3m1st/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7294921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8999e",
   "metadata": {},
   "source": [
    "### Encoder and Decoder definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c08d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "input_dim = 784  \n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "output_dim = input_dim\n",
    "\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "decoder = Decoder(latent_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73a7941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784  \n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41184a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/4lch3m1st/anaconda3/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1043: `configure_optimizers` must be implemented to be used with the Lightning Trainer\n"
     ]
    }
   ],
   "source": [
    "autoencoder = LitAutoEncoder(encoder, decoder)\n",
    "optimizer = autoencoder.configure_optimizers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "474c990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5995840",
   "metadata": {},
   "source": [
    "### Autoencoder class inheriting from pl.LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6775784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 128)\n",
    "        self.decoder = nn.Linear(128, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.decoder(x))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, _ = batch\n",
    "        recon_batch = self(data)\n",
    "        recon_batch = recon_batch.view(-1, 1, 28, 28)  # Reshape to match the shape of data\n",
    "        loss = F.mse_loss(recon_batch, data)\n",
    "        self.log('train_loss', loss)  # Log the training loss for visualization\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e158c6",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cb8f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `Autoencoder`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m Autoencoder()\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Set the number of epochs here\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(autoencoder, train_loader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m \n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    540\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/compile.py:132\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    131\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `Autoencoder`"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)  # Set the number of epochs here\n",
    "\n",
    "\n",
    "trainer.fit(autoencoder, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1b69c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/4lch3m1st/lightning_logs\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | fc      | Linear | 100 K \n",
      "1 | decoder | Linear | 101 K \n",
      "-----------------------------------\n",
      "201 K     Trainable params\n",
      "0         Non-trainable params\n",
      "201 K     Total params\n",
      "0.806     Total estimated model params size (MB)\n",
      "/Users/4lch3m1st/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1544eb5d869f4991a2980a2edf3259c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define your Autoencoder class inheriting from pl.LightningModule\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 128)\n",
    "        self.decoder = nn.Linear(128, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.decoder(x))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, _ = batch\n",
    "        recon_batch = self(data)\n",
    "        recon_batch = recon_batch.view(-1, 1, 28, 28)  # Reshape to match the shape of data\n",
    "        loss = F.mse_loss(recon_batch, data)\n",
    "        self.log('train_loss', loss)  # Log the training loss for visualization\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "# Load your data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize your Autoencoder model\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Initialize the Lightning Trainer and fit the model\n",
    "trainer = pl.Trainer(max_epochs=10)  # Set the number of epochs here\n",
    "trainer.fit(autoencoder, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36a56c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 9912422/9912422 [00:22<00:00, 437432.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 28881/28881 [00:00<00:00, 41880.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1648877/1648877 [00:02<00:00, 726040.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 3221259.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load data sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb422f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in ./anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (2023.3.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (4.11.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in ./anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (0.11.2)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.8.3)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f11485",
   "metadata": {},
   "source": [
    "### Define the test loop and implement the test_step method of the LightningModule to add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6b03333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through encoder and decoder\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step implementation\n",
    "        ...\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Test loop implementation\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"test_loss\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22c41ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Define the layers of the encoder here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass of the encoder\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Define the layers of the decoder here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass of the decoder\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f21f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through encoder and decoder\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b79cba",
   "metadata": {},
   "source": [
    "### Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b9da1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model does not contain trainable parameters\n"
     ]
    }
   ],
   "source": [
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "\n",
    "if list(autoencoder.parameters()):\n",
    "    print(\"Model contains trainable parameters\")\n",
    "else:\n",
    "    print(\"Model does not contain trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6492af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  # Importing the neural network module from PyTorch\n",
    "\n",
    "class Encoder(nn.Module):  # Defining a class named Encoder which is a subclass of nn.Module\n",
    "    def __init__(self):  \n",
    "        super(Encoder, self).__init__()  \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "                                                        \n",
    "        self.fc2 = nn.Linear(hidden_size, latent_size)  \n",
    "\n",
    "    def forward(self, x): \n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        return x \n",
    "class Decoder(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Decoder, self).__init__() \n",
    "        self.fc1 = nn.Linear(latent_size, hidden_size) \n",
    "                                                        \n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)   \n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = torch.sigmoid(self.fc2(x))  \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e15ab4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd9b3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ...\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "518b6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class YourModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(10, 2)  # Adding a simple linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define your training step\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define and return your optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Define and return your training dataloader\n",
    "        dataset = ...  # Your training dataset\n",
    "        dataloader = torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d56a1c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea3de190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | layer | Linear | 7.9 K \n",
      "---------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e66a69f30ae4d659e283d3387163436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to: /Users/4lch3m1st/lightning_logs/version_1/checkpoints/epoch=29-step=56250.ckpt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define your LightningModule\n",
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define your model architecture and other necessary components here\n",
    "        self.layer = nn.Linear(28 * 28, 10)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define your training step\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        self.log('train_loss', loss)  # log the training loss\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Define your train data loader\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "# Initialize lightningModule and Trainer\n",
    "model = MyModel()\n",
    "trainer = pl.Trainer(max_epochs=30)\n",
    "\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Get the file path to the saved checkpoint\n",
    "checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "# Print the file path\n",
    "print(\"Checkpoint saved to:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d63d4e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (layer): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# Load the LightningModule from the checkpoint\n",
    "model = MyModel.load_from_checkpoint(\"/Users/4lch3m1st/lightning_logs/version_1/checkpoints/epoch=29-step=56250.ckpt\")\n",
    "\n",
    "# Disable randomness, dropout, etc...\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "442f559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class MyLightningModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # Initialize your LightningModule\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define your training step\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer\n",
    "        pass\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "624ba7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model with in_dim=32 and out_dim=10\n",
    "model = LitModel(in_dim=32, out_dim=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35526cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MyLightningModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "model = MyLightningModule(learning_rate=0.001, another_parameter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e58bae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Specify the correct path to the checkpoint file\n",
    "checkpoint_path = (\"/Users/4lch3m1st/lightning_logs/version_1/checkpoints/epoch=29-step=56250.ckpt\")\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at path: {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4cbedec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the checkpoint: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Specify the file path to the checkpoint\n",
    "checkpoint_path = (\"/Users/4lch3m1st/lightning_logs/version_1/checkpoints/epoch=29-step=56250.ckpt\")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Print all keys in the checkpoint dictionary\n",
    "print(\"Keys in the checkpoint:\", checkpoint.keys())\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a39b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a14c1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=3,          \n",
    "    mode='min'           \n",
    ")\n",
    "trainer = pl.Trainer(callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c9bd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class AutoEncoder(LightningModule):\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "\n",
    "class CIFAR10Classifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        # init the pretrained LightningModule\n",
    "        self.feature_extractor = AutoEncoder.load_from_checkpoint(PATH)\n",
    "        self.feature_extractor.freeze()\n",
    "\n",
    "        # the autoencoder outputs a 100-dim representation and CIFAR-10 has 10 classes\n",
    "        self.classifier = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        representations = self.feature_extractor(x)\n",
    "        x = self.classifier(representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b9c1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: learning_rate=0.001, batch_size=32, num_epochs=10\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Training script with hyperparameters configuration')\n",
    "\n",
    "    # Add hyperparameters as command-line arguments\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for training')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs for training')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def main(args):\n",
    "    # Access hyperparameters from the args object\n",
    "    learning_rate = args.learning_rate\n",
    "    batch_size = args.batch_size\n",
    "    num_epochs = args.num_epochs\n",
    "\n",
    "   \n",
    "    print(f\"Training with hyperparameters: learning_rate={learning_rate}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse command-line arguments\n",
    "    args = parse_arguments()\n",
    "\n",
    "    # Call main function with parsed arguments\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "655b4384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(fast_dev_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdfdd25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 7 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(fast_dev_run=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eacde9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use only 10% of training data and 1% of val data\n",
    "trainer = Trainer(limit_train_batches=0.1, limit_val_batches=0.01)\n",
    "\n",
    "# use 10 batches of train and 5 batches of val\n",
    "trainer = Trainer(limit_train_batches=10, limit_val_batches=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faa5b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(num_sanity_val_steps=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bbbf5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class PrintWeightsSummary(Callback):\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        print(\"Weights Summary:\")\n",
    "        print(pl_module)\n",
    "\n",
    "        # Print summary of model's parameters\n",
    "        print(pl_module.summarize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bb42e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 9912422/9912422 [00:10<00:00, 949205.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 28881/28881 [00:00<00:00, 92515.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:01<00:00, 1281824.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 2306917.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/4lch3m1st/.pytorch/MNIST_data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])\n",
    "\n",
    "# Download and load the training set\n",
    "train_set = datasets.MNIST('~/.pytorch/MNIST_data/', train=True, download=True, transform=transform)\n",
    "\n",
    "# Download and load the test set\n",
    "test_set = datasets.MNIST('~/.pytorch/MNIST_data/', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for training and test sets\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e589bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Calculate the value you want to track\n",
    "        value = ...\n",
    "\n",
    "        # Log the value using self.log\n",
    "        self.log(\"some_value\", value)\n",
    "\n",
    "        # Return whatever value is needed\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ea8ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Calculate the value you want to track\n",
    "        value = ...\n",
    "\n",
    "        # Log the value with prog_bar=True\n",
    "        self.log(\"some_value\", value, prog_bar=True)\n",
    "\n",
    "        # Return whatever value is needed\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27a48d",
   "metadata": {},
   "source": [
    "### Launch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d0bfb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Obtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Obtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/c1/0a/a8c0f403b2189f5d3e490778ead51924b56fa30a35f6e444b3702e28c8c8/grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.11/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./anaconda3/lib/python3.11/site-packages (from tensorboard) (1.24.3)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Obtaining dependency information for protobuf!=4.24.0,>=3.19.6 from https://files.pythonhosted.org/packages/1e/40/2eb2bf643d4b060b1602a25748b48d75431f4951be2470f8ae136952b3d3/protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./anaconda3/lib/python3.11/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in ./anaconda3/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/b7/85/dabeaf902892922777492e1d253bb7e1264cadce3cea932f7ff599e53fea/tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./anaconda3/lib/python3.11/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m606.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.0/404.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m244.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-data-server, protobuf, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.62.1 protobuf-5.26.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1e9365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cf9d7c823bf67de8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cf9d7c823bf67de8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51bb81ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69d05b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(LightningModule):\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dcffefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/4lch3m1st/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e16c28a53467f8231549366af9abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define LightningModule class \n",
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define model architecture\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "\n",
    "        return x  \n",
    "\n",
    "data_loader = DataLoader(dataset)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "# Make predictions using the predict method\n",
    "predictions = trainer.predict(model, dataloaders=data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e0efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
